# app.py â€” Streamlit Cloud Ready (PAT + REST + SSE íŒŒì„œ + ì˜êµ¬ íˆìŠ¤í† ë¦¬)
# í•„ìš” íŒ¨í‚¤ì§€: streamlit, requests, pandas, snowflake-connector-python

import json, re, requests, pandas as pd, streamlit as st
from snowflake import connector as sf

# ---------------------------
# App config
# ---------------------------
st.set_page_config(page_title="â„ï¸ SNOWì±—ë´‡ (Cortex Agents)", page_icon="â„ï¸", layout="wide")
st.title("â„ï¸ SNOWì±—ë´‡ - Cortex Agents (Cloud)")
st.caption("Cortex Search for calls Â· Analyst for metrics Â· Persistent chat")

# ---------------------------
# Secrets (Streamlit Cloudì— ì„¤ì •)
# ---------------------------
SF = st.secrets["snowflake"]
ACCOUNT_BASE = SF["account_base"].rstrip("/")  # https://<acct>.<region>.<cloud>.snowflakecomputing.com
PAT          = SF["pat"]                       # Programmatic Access Token
ROLE         = SF.get("role", "SALES_INTELLIGENCE_RL")
WAREHOUSE    = SF.get("warehouse", "SALES_INTELLIGENCE_WH")
DATABASE     = SF.get("database",  "SALES_INTELLIGENCE")
SCHEMA       = SF.get("schema",    "DATA")

# (SQL ì‹¤í–‰ìš© ì»¤ë„¥í„° ìê²© â€” ìˆìœ¼ë©´ ê²°ê³¼ í…Œì´ë¸”ê¹Œì§€ í‘œì‹œ/ë³´ì¡´)
SF_USER      = SF.get("user")
SF_PASSWORD  = SF.get("password")
SF_ACCOUNT   = SF.get("account")   # ì˜ˆ: rub23142.us-west-2.aws

# ëª¨ë¸ ì„ íƒ(ë¦¬ì „ì— ë§ê²Œ)
MODEL_NAME = st.sidebar.selectbox("Model", ["mistral-large2", "llama3.3-70b"], index=0)

# í•­ìƒ ì ìš©ë˜ëŠ” ë™ì‘
AUTO_RUN_SQL = True
AUTO_QUALIFY = True

# ë¦¬ì†ŒìŠ¤ ì´ë¦„(FQN)
CORTEX_SEARCH_SERVICE = "SALES_INTELLIGENCE.DATA.SALES_CONVERSATION_SEARCH"
SEMANTIC_MODEL_FILE   = "@SALES_INTELLIGENCE.DATA.MODELS/sales_metrics_model.yaml"
API_ENDPOINT          = f"{ACCOUNT_BASE}/api/v2/cortex/agent:run"

# ---------------------------
# FQN ë³´ì •
# ---------------------------
_TABLE_FQN = {
    "SALES_CONVERSATIONS": "SALES_INTELLIGENCE.DATA.SALES_CONVERSATIONS",
    "SALES_METRICS":       "SALES_INTELLIGENCE.DATA.SALES_METRICS",
}
def qualify_sql(sql: str) -> str:
    s = (sql or "").strip().rstrip(";")
    for short, fqn in _TABLE_FQN.items():
        s = re.sub(rf"(?<!\.)\b{short}\b", fqn, s, flags=re.IGNORECASE)
    return s

# ---------------------------
# Snowflake SQL ì‹¤í–‰(Connector)
# ---------------------------
@st.cache_resource(show_spinner=False)
def get_conn():
    if not (SF_USER and SF_PASSWORD and SF_ACCOUNT):
        return None
    return sf.connect(
        user=SF_USER, password=SF_PASSWORD, account=SF_ACCOUNT,
        warehouse=WAREHOUSE, database=DATABASE, schema=SCHEMA, role=ROLE,
        authenticator="snowflake",
        session_parameters={"CLIENT_SESSION_KEEP_ALIVE": True},
    )

def run_sql(sql: str) -> pd.DataFrame | None:
    conn = get_conn()
    if conn is None:
        st.info("ğŸ” secretsì— user/password/accountê°€ ì—†ì–´ SQL ì‹¤í–‰ì€ ìƒëµí•©ë‹ˆë‹¤.")
        return None
    try:
        q = qualify_sql(sql) if AUTO_QUALIFY else sql
        with conn.cursor() as cur:
            cur.execute(q)
            rows = cur.fetchall()
            cols = [d[0] for d in cur.description]
        return pd.DataFrame(rows, columns=cols)
    except Exception as e:
        st.error(f"âŒ SQL ì‹¤í–‰ ì—ëŸ¬: {e}")
        st.code(sql, language="sql")
        return None

# ---------------------------
# ë¼ìš°íŒ… íœ´ë¦¬ìŠ¤í‹± (Search â†” SQL)
# ---------------------------
_SEARCH_HINTS = {
    "call","calls","conversation","conversations","transcript","transcripts",
    "meeting","meetings","qbr","summary","summarize","tell me about","what did"
}
_SQL_HINTS = {
    "how many","count","sum","avg","average","total","compare","vs","trend","won","lost",
    "revenue","value","pipeline","close rate","ratio","by ","group by","top ","rank","percent"
}
def detect_intent(q: str) -> str:
    ql = (q or "").lower()
    if any(k in ql for k in _SEARCH_HINTS) and not any(k in ql for k in _SQL_HINTS):
        return "Search"
    if any(k in ql for k in _SQL_HINTS):
        return "SQL"
    return "Auto"

# ---------------------------
# í…ìŠ¤íŠ¸ ì •ë¦¬
# ---------------------------
def _normalize_text(s: str) -> str:
    s = s.replace("\r\n", "\n")
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()

# ---------------------------
# Agents REST í˜¸ì¶œ (JSON + SSE ëª¨ë‘ ì§€ì›)
# ---------------------------
def build_headers():
    # í—¤ë” ìµœì†Œí™”: ë¹„ASCII ê°€ëŠ¥ì„± ì°¨ë‹¨ (í•„ìˆ˜ë§Œ ë‚¨ê¹€)
    return {
        "Authorization": f"Bearer {PAT}",
        "X-Snowflake-Authorization-Token-Type": "PROGRAMMATIC_ACCESS_TOKEN",
        "Accept": "application/json",          # ìš°ì„  SSE ë¹„í™œì„±í™”
        "Content-Type": "application/json",
        # â€» ì—­í• /DB/ìŠ¤í‚¤ë§ˆ/ì›¨ì–´í•˜ìš°ìŠ¤ í—¤ë”ëŠ” ì„ì‹œë¡œ ì œê±°
        # PATì˜ DEFAULT_ROLE/ì»¨í…ìŠ¤íŠ¸ë¡œ ë™ì‘í•˜ë„ë¡
    }

def build_payload(user_text: str, max_results: int = 5) -> dict:
    intent = detect_intent(user_text)
    use_search = intent in ("Auto", "Search")
    use_sql    = intent in ("Auto", "SQL")

    tools, tool_resources = [], {}
    if use_search:
        tools.append({"tool_spec": {"type": "cortex_search", "name": "search1"}})
        tool_resources["search1"] = {
            "name": CORTEX_SEARCH_SERVICE,
            "max_results": max_results,
            "id_column": "conversation_id",
        }
    if use_sql:
        tools.append({"tool_spec": {"type": "cortex_analyst_text_to_sql", "name": "analyst1"}})
        tool_resources["analyst1"] = {"semantic_model_file": SEMANTIC_MODEL_FILE}

    if intent == "Search":
        sys_text = ("You are a helpful sales conversation assistant. "
                    "Prefer cortex_search to find relevant call transcripts and summarize them. "
                    "Return a concise natural language summary and include brief citations.")
    elif intent == "SQL":
        sys_text = ("You are a helpful SQL analyst. "
                    "Prefer cortex_analyst_text_to_sql for metrics/aggregations. "
                    "Also return a short natural language answer summarizing the result.")
    else:
        sys_text = ("You are a helpful assistant. "
                    "For conversations/calls/transcripts, use cortex_search to retrieve and summarize. "
                    "For metrics/counts/aggregates, use cortex_analyst_text_to_sql to generate SQL. "
                    "Always include a short natural language answer.")

    return {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": [{"type": "text", "text": sys_text}]},
            {"role": "user",   "content": [{"type": "text", "text": user_text}]}
        ],
        "tools": tools,
        "tool_resources": tool_resources,
        # í•„ìš” ì‹œ ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”ë¥¼ ì‹œë„í•˜ë ¤ë©´(ì¼ë¶€ í™˜ê²½):
        # "stream": False
    }

# --- SSE í…ìŠ¤íŠ¸ â†’ ì´ë²¤íŠ¸ ë°°ì—´ íŒŒì„œ
def parse_sse_text(s: str) -> list:
    events, cur = [], {"event": None, "data": ""}
    for raw in (s or "").splitlines():
        line = raw.strip("\n")
        if not line:
            # ì´ë²¤íŠ¸ ê²½ê³„
            if cur["event"]:
                # dataëŠ” JSONì¼ ìˆ˜ë„ ìˆê³  ì•„ë‹ ìˆ˜ë„
                try:
                    data_obj = json.loads(cur["data"]) if cur["data"] else {}
                except Exception:
                    data_obj = {"raw": cur["data"]}
                # ì—ì´ì „íŠ¸ì˜ message.delta í˜•íƒœë¡œ ì •ê·œí™”
                if cur["event"].startswith("message"):
                    ev = {"event": "message.delta", "data": {"delta": data_obj.get("delta", data_obj)}}
                elif cur["event"] == "error":
                    ev = {"event": "error", "data": data_obj}
                else:
                    ev = {"event": cur["event"], "data": data_obj}
                events.append(ev)
            cur = {"event": None, "data": ""}
            continue
        if line.startswith("event:"):
            cur["event"] = line[len("event:"):].strip()
        elif line.startswith("data:"):
            payload = line[len("data:"):].strip()
            cur["data"] += (payload if not cur["data"] else "\n" + payload)
    # ë§ˆì§€ë§‰ ì”ì—¬
    if cur["event"]:
        try:
            data_obj = json.loads(cur["data"]) if cur["data"] else {}
        except Exception:
            data_obj = {"raw": cur["data"]}
        events.append({"event": cur["event"], "data": {"delta": data_obj}})
    return events

def call_agents_rest(payload: dict, timeout: int = 60):
    # í—¤ë” ë¼í‹´-1 ì¸ì½”ë”© ê²€ì¦(ë””ë²„ê·¸ìš©)
    hdr = build_headers()
    for k, v in list(hdr.items()):
        if isinstance(v, str):
            try:
                v.encode("latin-1")
            except UnicodeEncodeError as e:
                # ë§Œì•½ ì—¬ê¸° ê±¸ë¦¬ë©´ ë°”ë¡œ ì•Œë ¤ì¤˜
                st.error(f"Header not latin-1 encodable: {k}={v!r} ({e})")
                raise

    # ë³¸ë¬¸ì€ json= ìœ¼ë¡œ ë³´ë‚´ UTF-8 ì²˜ë¦¬ ë³´ì¥
    r = requests.post(API_ENDPOINT, headers=hdr, json=payload, timeout=timeout)
    if r.status_code != 200:
        raise RuntimeError(f"HTTP {r.status_code} - {r.reason}\n{r.text[:2000]}")
    try:
        return r.json()
    except Exception:
        return json.loads(r.text)

# ---------------------------
# ì‘ë‹µ íŒŒì‹±
# ---------------------------
def _pull_from_tool_result(result_obj, citations, set_sql_fn):
    if not isinstance(result_obj, dict): return
    j = result_obj.get("json")
    if isinstance(j, str):
        try: j = json.loads(j)
        except: j = None
    if not isinstance(j, dict): return
    for k in ("sql","generated_sql","sql_query"):
        v = j.get(k)
        if isinstance(v, str) and v.strip():
            set_sql_fn(v); break
    for s in j.get("searchResults") or j.get("results") or []:
        if isinstance(s, dict):
            citations.append({"source_id": s.get("source_id",""),
                              "doc_id":    s.get("doc_id","") or s.get("id","")})

def parse_json_response(obj: dict):
    text_parts, citations = [], []
    sql_holder = {"v": None}
    def set_sql(v):
        if v and isinstance(v,str) and v.strip(): sql_holder["v"] = v
    def pull(content):
        if isinstance(content, dict): content = [content]
        if not isinstance(content, list): return
        for item in content:
            t = item.get("type")
            if t == "text":
                txt = item.get("text")
                if isinstance(txt,str): text_parts.append(txt)
            elif t == "tool_results":
                for r in item.get("tool_results",{}).get("content",[]):
                    _pull_from_tool_result(r, citations, set_sql)
    for key in ("output","message","response","data"):
        node = obj.get(key)
        if isinstance(node, dict) and "content" in node:
            pull(node["content"])
    if not text_parts and "content" in obj:
        pull(obj["content"])
    text = _normalize_text("".join(text_parts))
    return text, sql_holder["v"], citations

def parse_events_response(events: list):
    text_parts, citations = [], []
    sql_holder = {"v": None}
    def set_sql(v):
        if v and isinstance(v,str) and v.strip() and not sql_holder["v"]:
            sql_holder["v"] = v
    for ev in events:
        if not isinstance(ev, dict): continue
        if ev.get("event") == "message.delta":
            delta = ev.get("data",{}).get("delta",{})
            # delta.content: [{"type":"text","text":"..."}, {"type":"tool_results",...}]
            content = delta.get("content") or []
            if isinstance(content, dict): content = [content]
            for item in content:
                if item.get("type") == "text":
                    t = item.get("text")
                    if isinstance(t,str): text_parts.append(t)
                elif item.get("type") == "tool_results":
                    for r in item.get("tool_results",{}).get("content",[]):
                        _pull_from_tool_result(r, citations, set_sql)
    text = _normalize_text("".join(text_parts))
    return text, sql_holder["v"], citations

def parse_any(resp):
    if resp is None: return "", None, []
    if isinstance(resp, dict):
        return parse_json_response(resp)
    if isinstance(resp, list):
        return parse_events_response(resp)
    if isinstance(resp, str) and resp.startswith("event:"):
        return parse_events_response(parse_sse_text(resp))
    # raw/unknown
    raw = resp.get("raw") if isinstance(resp, dict) else None
    return (raw or ""), None, []

# ---------------------------
# Chat state (íˆìŠ¤í† ë¦¬ + í…Œì´ë¸”/ì¸ìš© ë³´ì¡´)
# ---------------------------
if "messages" not in st.session_state:
    st.session_state.messages = []

# íˆìŠ¤í† ë¦¬ ë Œë”ë§
for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m.get("content",""))
        for t in m.get("tables", []):
            st.write(f"### {t.get('title','Query Result')}")
            st.dataframe(pd.DataFrame(t["data"]), use_container_width=True)
        for ex in m.get("expanders", []):
            with st.expander(ex.get("header","Details")):
                st.write(ex.get("body",""))

# ---------------------------
# ì…ë ¥ & í˜¸ì¶œ
# ---------------------------
query = st.chat_input("Ask anything. (ex) Tell me about the call with SecureBank?  /  How many deals did Sarah Johnson win vs lost?)")
if not query:
    st.stop()

# ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥/í‘œì‹œ
st.session_state.messages.append({"role":"user","content":query})
with st.chat_message("user"):
    st.markdown(query)

# ì—ì´ì „íŠ¸ í˜¸ì¶œ
with st.spinner("Calling Cortex Agents..."):
    try:
        body = call_agents_rest(build_payload(query, max_results=5), timeout=90)
    except Exception as e:
        st.error(str(e))
        st.stop()

text, sql, citations = parse_any(body)

# ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ êµ¬ì„± + ì²¨ë¶€ë“¤(í…Œì´ë¸”/expander)ë„ í•¨ê»˜ ì €ì¥
assistant_chunks, tables_to_persist, expanders_to_persist = [], [], []

if text:
    assistant_chunks.append(text)

if sql:
    assistant_chunks.append("### Generated SQL\n```sql\n" + sql.strip() + "\n```")
    if AUTO_RUN_SQL:
        df = run_sql(sql)
        if df is not None:
            st.write("### Query Result")
            st.dataframe(df, use_container_width=True)
            # íˆìŠ¤í† ë¦¬ì— ì €ì¥
            df_safe = df.astype(str)
            tables_to_persist.append({
                "title": "Query Result",
                "data": df_safe.to_dict(orient="records")
            })
            assistant_chunks.append(f"_Query returned **{len(df)}** row(s)._")

if citations:
    ids = [c.get("doc_id","") for c in citations if c.get("doc_id")]
    if ids:
        assistant_chunks.append("**Citations:** " + ", ".join(f"`{i}`" for i in ids))
        st.markdown("### Citations")
    for doc_id in ids:
        if not get_conn():  # ì»¤ë„¥í„° ì—†ìœ¼ë©´ ì „ë¬¸ ë¯¸ë¦¬ë³´ê¸° ìƒëµ
            continue
        preview_sql = f"""
        SELECT CONVERSATION_ID, CUSTOMER_NAME, SALES_REP, DEAL_STAGE, CONVERSATION_DATE, DEAL_VALUE, PRODUCT_LINE, TRANSCRIPT_TEXT
        FROM SALES_INTELLIGENCE.DATA.SALES_CONVERSATIONS
        WHERE CONVERSATION_ID = '{doc_id.replace("'", "''")}'
        """
        dfp = run_sql(preview_sql)
        if dfp is None or dfp.empty:
            continue
        row = dfp.iloc[0].to_dict()
        header = f"[{row.get('CONVERSATION_ID','')}] {row.get('CUSTOMER_NAME','')} Â· {row.get('SALES_REP','')} Â· {row.get('DEAL_STAGE','')} Â· {row.get('CONVERSATION_DATE','')}"
        body_text = row.get("TRANSCRIPT_TEXT","(no transcript)")
        with st.expander(header):
            st.write(body_text)
        expanders_to_persist.append({"header": header, "body": body_text})

assistant_text = "\n\n".join(assistant_chunks).strip() if assistant_chunks else "_No answer returned._"
assistant_msg = {"role":"assistant","content":assistant_text,"tables":tables_to_persist,"expanders":expanders_to_persist}
st.session_state.messages.append(assistant_msg)

with st.chat_message("assistant"):
    st.markdown(assistant_text)
    for t in tables_to_persist:
        st.write(f"### {t.get('title','Query Result')}")
        st.dataframe(pd.DataFrame(t["data"]), use_container_width=True)
    for ex in expanders_to_persist:
        with st.expander(ex.get("header","Details")):
            st.write(ex.get("body",""))
